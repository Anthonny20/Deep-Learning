{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec1c786",
   "metadata": {},
   "source": [
    "Lab 1: Intro to Pytorch and Music Generation with RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e0212e",
   "metadata": {},
   "source": [
    "In this lab, you'll get exposure to using PyTorch and learn how it can be used for deep learning. Go through the code and run each cell.\n",
    "Along the way, you'll encouter several TODO blocks -- follow the instructions to fill them out before running those cells and continuing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e2b4af",
   "metadata": {},
   "source": [
    "Part1: Intro to PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580710f5",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning library known for its flexibility and ease of use. Here we'll learn how computations are represented and how to define a simple neural network in PyTorch. For all the labs in Introduction to Deep Learning 2025, there will be a Pytorch version available.\n",
    "\n",
    "Let's install PyTorch and a couple of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "800dfd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anthonny.paz\\Documents\\GitHub\\Deep-Learning\\deep-learn-3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3044f76a",
   "metadata": {},
   "source": [
    "1.1 What is PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dabde5",
   "metadata": {},
   "source": [
    "Pytorch is a machine learning library, like TensorFlow. At it's core, PyTorch provides an interface for creating and manipulating tensors, which are data structures that you can think of as multi-dimensional arrays. Tensors are represented as n-dimensional arrays of base datatypes such as a string or integer -- they provide a way to generalize vectors and matrices to higher dimensions. PyTorch provides the ability to perform computation on these tensors, define neural networks, and train them efficiently.\n",
    "\n",
    "The shape of a PyTorch tensor defines its number of dimensions and the size of each dimension. The ndim or dim of a Pytorch tensor provides the number of dimensions (n-dimensions) --  this is equivalent to the tensor's rank(as is used in TensorFlow), and you can also think of this as the tensor's order or degree.\n",
    "\n",
    "Let's start by changing some tensors and inspecting their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef58d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Integer` is a 0-d Tensor: 1234\n",
      "`Decimal` is a 0-d Tensor: 3.1415927410125732\n"
     ]
    }
   ],
   "source": [
    "integer = torch.tensor(1234)\n",
    "decimal = torch.tensor(3.14159265359)\n",
    "\n",
    "print(f\"`Integer` is a {integer.ndim}-d Tensor: {integer}\")\n",
    "print(f\"`Decimal` is a {decimal.ndim}-d Tensor: {decimal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5936e56",
   "metadata": {},
   "source": [
    "Vectors and lists can be used to create 1-d tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d09ba4e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`Fibonacci` is a 1-d Tensor with shape: torch.Size([6])\n",
      "`count_to_100` is a 1-d Tensor with shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "fibonacci = torch.tensor([1, 1, 2, 3, 5, 8])\n",
    "count_to_100 = torch.tensor(range(100))\n",
    "\n",
    "print(f\"`Fibonacci` is a {fibonacci.ndim}-d Tensor with shape: {fibonacci.shape}\")\n",
    "print(f\"`count_to_100` is a {count_to_100.ndim}-d Tensor with shape: {count_to_100.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d78828",
   "metadata": {},
   "source": [
    "Next, let's create 2-d (i.e., matrices) and higher-rank tensors. In image processing and Computer Vision, we will use 4-d Tensors with dimensions corresponding to batch size, number of color channels, image height, and image width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "697b488c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images is a 4-d Tensor with shape: torch.Size([10, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "### Defining higher-order Tensors ###\n",
    "'''TODO: Define a 2-d Tensor'''\n",
    "matrix = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "assert isinstance(matrix, torch.Tensor), \"matrix must be a torch Tensor object\"\n",
    "assert matrix.ndim == 2\n",
    "\n",
    "'''TODO: Define a 4-d Tensor'''\n",
    "# Use torch.zeros to initialize a 4-d Tensor of zeros with size 10 x 3 x 256 x 256.\n",
    "#   You can think of this as 10 images where each image is RGB 256 X 256.\n",
    "images = torch.zeros(10, 3, 256, 256)\n",
    "\n",
    "assert isinstance(images, torch.Tensor), \"images must be a torch Tensor object\"\n",
    "assert images.ndim == 4, \"images must have 4 dimensions\"\n",
    "assert images.shape == (10, 3, 256, 256), \"images is incorrect shape\"\n",
    "print(f\"images is a {images.ndim}-d Tensor with shape: {images.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc9fa8a",
   "metadata": {},
   "source": [
    "As you have seen, the shape of a tensor provides the number of elements in each tensor dimension. The shape is quite useful, and we'll use it often. You can also use slicing to access subtensors within a higher-rank tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25a0db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_vector = matrix[1]\n",
    "column_vector = matrix[:,1]\n",
    "scalar = matrix[0, 1]\n",
    "\n",
    "print(f\"row_vector\": {r})\n",
    "print(column_vector)\n",
    "print(scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dbcc73cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6])\n",
      "tensor([2, 5])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(row_vector)\n",
    "print(column_vector)\n",
    "print(scalar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learn-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
